<!DOCTYPE html> <html><head>
		<title>论文：Image amodal completion _A survey</title>
		<base href="../">
		<meta id="root-path" root-path="../">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=1.0, maximum-scale=5.0">
		<meta charset="UTF-8">
		<meta name="description" content="视觉 - 论文：Image amodal completion _A survey">
		<meta property="og:title" content="论文：Image amodal completion _A survey">
		<meta property="og:description" content="视觉 - 论文：Image amodal completion _A survey">
		<meta property="og:type" content="website">
		<meta property="og:url" content="https://www.glacierchen.asia/非模态分割/论文：image-amodal-completion-_a-survey.html">
		<meta property="og:image" content="undefined">
		<meta property="og:site_name" content="视觉">
		<meta name="author" content="Glacier"><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://www.glacierchen.asia/lib/rss.xml"><script async="" id="webpage-script" src="lib/scripts/webpage.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script type="module" async="" id="graph-view-script" src="lib/scripts/graph-view.js"></script><script async="" id="graph-wasm-script" src="lib/scripts/graph-wasm.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="graph-render-worker-script" src="lib/scripts/graph-render-worker.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="tinycolor-script" src="lib/scripts/tinycolor.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="pixi-script" src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.4.0/pixi.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><script async="" id="minisearch-script" src="https://cdn.jsdelivr.net/npm/minisearch@6.3.0/dist/umd/index.min.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><link rel="icon" href="lib/media/favicon.png"><script async="" id="graph-data-script" src="lib/scripts/graph-data.js" onload="this.onload=null;this.setAttribute(&quot;loaded&quot;, &quot;true&quot;)"></script><style>body{--line-width:40em;--line-width-adaptive:40em;--file-line-width:40em;--sidebar-width:min(20em, 80vw);--collapse-arrow-size:11px;--tree-horizontal-spacing:0.6em;--tree-vertical-spacing:0.6em;--sidebar-margin:12px}.sidebar{height:100%;min-width:calc(var(--sidebar-width) + var(--divider-width-hover));max-width:calc(var(--sidebar-width) + var(--divider-width-hover));font-size:14px;z-index:10;position:relative;overflow:hidden;transition:min-width ease-in-out,max-width ease-in-out;transition-duration:.2s;contain:size}.sidebar-left{left:0}.sidebar-right{right:0}.sidebar.is-collapsed{min-width:0;max-width:0}body.floating-sidebars .sidebar{position:absolute}.sidebar-content{height:100%;min-width:calc(var(--sidebar-width) - var(--divider-width-hover));top:0;padding:var(--sidebar-margin);padding-top:4em;line-height:var(--line-height-tight);background-color:var(--background-secondary);transition:background-color,border-right,border-left,box-shadow;transition-duration:var(--color-fade-speed);transition-timing-function:ease-in-out;position:absolute;display:flex;flex-direction:column}.sidebar:not(.is-collapsed) .sidebar-content{min-width:calc(max(100%,var(--sidebar-width)) - 3px);max-width:calc(max(100%,var(--sidebar-width)) - 3px)}.sidebar-left .sidebar-content{left:0;border-top-right-radius:var(--radius-l);border-bottom-right-radius:var(--radius-l)}.sidebar-right .sidebar-content{right:0;border-top-left-radius:var(--radius-l);border-bottom-left-radius:var(--radius-l)}.sidebar:has(.sidebar-content:empty):has(.topbar-content:empty){display:none}.sidebar-topbar{height:2em;width:var(--sidebar-width);top:var(--sidebar-margin);padding-inline:var(--sidebar-margin);z-index:1;position:fixed;display:flex;align-items:center;transition:width ease-in-out;transition-duration:inherit}.sidebar.is-collapsed .sidebar-topbar{width:calc(2.3em + var(--sidebar-margin) * 2)}.sidebar .sidebar-topbar.is-collapsed{width:0}.sidebar-left .sidebar-topbar{left:0}.sidebar-right .sidebar-topbar{right:0}.topbar-content{overflow:hidden;overflow:clip;width:100%;height:100%;display:flex;align-items:center;transition:inherit}.sidebar.is-collapsed .topbar-content{width:0;transition:inherit}.clickable-icon.sidebar-collapse-icon{background-color:transparent;color:var(--icon-color-focused);padding:0!important;margin:0!important;height:100%!important;width:2.3em!important;margin-inline:0.14em!important;position:absolute}.sidebar-left .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);right:var(--sidebar-margin)}.sidebar-right .clickable-icon.sidebar-collapse-icon{transform:rotateY(180deg);left:var(--sidebar-margin)}.clickable-icon.sidebar-collapse-icon svg.svg-icon{width:100%;height:100%}.sidebar-section-header{margin:0 0 1em 0;text-transform:uppercase;letter-spacing:.06em;font-weight:600}body{transition:background-color var(--color-fade-speed) ease-in-out}.webpage-container{display:flex;flex-direction:row;height:100%;width:100%;align-items:stretch;justify-content:center}.document-container{opacity:1;flex-basis:100%;max-width:100%;width:100%;height:100%;display:flex;flex-direction:column;align-items:center;transition:opacity .2s ease-in-out;contain:inline-size}.hide{opacity:0;transition:opacity .2s ease-in-out}.document-container>.markdown-preview-view{margin:var(--sidebar-margin);margin-bottom:0;width:100%;width:-webkit-fill-available;width:-moz-available;width:fill-available;background-color:var(--background-primary);transition:background-color var(--color-fade-speed) ease-in-out;border-top-right-radius:var(--window-radius,var(--radius-m));border-top-left-radius:var(--window-radius,var(--radius-m));overflow-x:hidden!important;overflow-y:auto!important;display:flex!important;flex-direction:column!important;align-items:center!important;contain:inline-size}.document-container>.markdown-preview-view>.markdown-preview-sizer{padding-bottom:80vh!important;width:100%!important;max-width:var(--line-width)!important;flex-basis:var(--line-width)!important;transition:background-color var(--color-fade-speed) ease-in-out;contain:inline-size}.markdown-rendered img:not([width]),.view-content img:not([width]){max-width:100%;outline:0}.document-container>.view-content.embed{display:flex;padding:1em;height:100%;width:100%;align-items:center;justify-content:center}.document-container>.view-content.embed>*{max-width:100%;max-height:100%;object-fit:contain}:has(> :is(.math,table)){overflow-x:auto!important}.document-container>.view-content{overflow-x:auto;contain:content;padding:0;margin:0;height:100%}.scroll-highlight{position:absolute;width:100%;height:100%;pointer-events:none;z-index:1000;background-color:hsla(var(--color-accent-hsl),.25);opacity:0;padding:1em;inset:50%;translate:-50% -50%;border-radius:var(--radius-s)}</style><script defer="">async function loadIncludes(){if("file:"!=location.protocol){let e=document.querySelectorAll("include");for(let t=0;t<e.length;t++){let o=e[t],l=o.getAttribute("src");try{const e=await fetch(l);if(!e.ok){console.log("Could not include file: "+l),o?.remove();continue}let t=await e.text(),n=document.createRange().createContextualFragment(t),i=Array.from(n.children);for(let e of i)e.classList.add("hide"),e.style.transition="opacity 0.5s ease-in-out",setTimeout((()=>{e.classList.remove("hide")}),10);o.before(n),o.remove(),console.log("Included file: "+l)}catch(e){o?.remove(),console.log("Could not include file: "+l,e);continue}}}else{if(document.querySelectorAll("include").length>0){var e=document.createElement("div");e.id="error",e.textContent="Web server exports must be hosted on an http / web server to be viewed correctly.",e.style.position="fixed",e.style.top="50%",e.style.left="50%",e.style.transform="translate(-50%, -50%)",e.style.fontSize="1.5em",e.style.fontWeight="bold",e.style.textAlign="center",document.body.appendChild(e),document.querySelector(".document-container")?.classList.remove("hide")}}}document.addEventListener("DOMContentLoaded",(()=>{loadIncludes()}));let isFileProtocol="file:"==location.protocol;function waitLoadScripts(e,t){let o=e.map((e=>document.getElementById(e+"-script"))),l=0;!function e(){let n=o[l];l++,n&&"true"!=n.getAttribute("loaded")||l<o.length&&e(),l<o.length?n.addEventListener("load",e):t()}()}</script><link rel="stylesheet" href="lib/styles/obsidian.css"><link rel="preload" href="lib/styles/other-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/other-plugins.css"></noscript><link rel="stylesheet" href="lib/styles/theme.css"><link rel="preload" href="lib/styles/global-variable-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/global-variable-styles.css"></noscript><link rel="preload" href="lib/styles/supported-plugins.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/supported-plugins.css"></noscript><link rel="preload" href="lib/styles/main-styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel="stylesheet" href="lib/styles/main-styles.css"></noscript></head><body class="publish css-settings-manager theme-dark show-inline-title show-ribbon background-settings-workplace-theme-light-leaf background-settings-workplace-theme-dark-in-the-sky background-image-settings-markdown-page-random-default notebook-liked-markdown-page-grid-notebook-1 panel-page-bg-theme-dark-sea layout-style-options-default bt-default-unordered-list default-ol-list-marker list-no-border folder-style-change-options-colorful-default folder-colorful-one table-width-100 rainbow-tag style-options-for-admonition-plugin style-options-for-buttons-plugin dataview-list-style-pacman quiet-outline-optimize code-theme-custom whole-code-wrap obsidian-themepocalypse is-focused"><script defer="">let theme=localStorage.getItem("theme")||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light");"dark"==theme?(document.body.classList.add("theme-dark"),document.body.classList.remove("theme-light")):(document.body.classList.add("theme-light"),document.body.classList.remove("theme-dark")),window.innerWidth<480?document.body.classList.add("is-phone"):window.innerWidth<768?document.body.classList.add("is-tablet"):window.innerWidth<1024?document.body.classList.add("is-small-screen"):document.body.classList.add("is-large-screen")</script><div class="webpage-container workspace"><div class="sidebar-left sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="search-input-container"><input enterkeyhint="search" type="search" spellcheck="false" placeholder="Search..."><div class="search-input-clear-button" aria-label="Clear search"></div></div><include src="lib/html/file-tree.html"></include></div><script defer="">let ls = document.querySelector(".sidebar-left"); ls.classList.add("is-collapsed"); if (window.innerWidth > 768) ls.classList.remove("is-collapsed"); ls.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-left-width"));</script></div><div class="document-container markdown-reading-view hide"><div class="markdown-preview-view markdown-rendered allow-fold-headings allow-fold-lists is-readable-line-width"><style id="MJX-CHTML-styles"></style><div class="markdown-preview-sizer markdown-preview-section"><h1 class="page-title heading inline-title" id="论文：Image amodal completion _A survey">论文：Image amodal completion _A survey</h1><div class="heading-wrapper"><h2 data-heading="1. Introduction" dir="auto" class="heading" id="1._Introduction"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. Introduction</h2><div class="heading-children"><div><p dir="auto">occlusion handling problem</p></div></div></div><div class="heading-wrapper"><h2 data-heading="2.Taxonomy of problems" dir="auto" class="heading" id="2.Taxonomy_of_problems"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2.Taxonomy of problems</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="1. Amodal Shape Completion（非模态形状补全）" dir="auto" class="heading" id="1._Amodal_Shape_Completion（非模态形状补全）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. Amodal Shape Completion（非模态形状补全）</h3><div class="heading-children"><div><p dir="auto">非模态形状补全是指利用部分视觉证据推测物体的完整形状，包括被遮挡部分的形状推测。这一任务的目标是恢复物体的整体轮廓，从而更好地理解场景中的物体大小和相对深度。为了实现这一目标，主要采用以下几种方法：</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Amodal Instance Segmentation（非模态实例分割）</strong>：
<ul>
<li data-line="2" dir="auto">这是最常用的非模态形状补全技术，其目标是预测一个实例的完整分割掩码（包括可见和不可见部分）。</li>
<li data-line="3" dir="auto">例如，Mask R-CNN 模型经过扩展后可以实现非模态形状补全，而不是仅仅关注可见部分的分割。</li>
<li data-line="4" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>根据是否结合目标检测技术，可以将这些方法分为两大类：
<ol>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>结合目标检测的非模态实例分割</strong>：
<ul>
<li data-line="6" dir="auto">首先使用目标检测方法获取目标所在区域（如边界框），然后在候选区域内进行像素级分割，生成物体的分割掩码。</li>
<li data-line="7" dir="auto">例如，Li 和 Malik（2016）提出的 IBBE 方法，通过迭代地调整边界框，逐步推测物体的完整形状。</li>
</ul>
</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>不结合目标检测的非模态实例分割</strong>：
<ul>
<li data-line="9" dir="auto">直接在像素级别进行形状补全，而不依赖于目标检测框。</li>
<li data-line="10" dir="auto">例如，基于深度学习的 AmodalMask（Zhu et al., 2017）模型通过输入图像块预测被遮挡物体的非模态掩码。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li data-line="11" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>几何形状补全方法</strong>：
<ul>
<li data-line="13" dir="auto">一些早期的研究尝试使用几何模型（如 Euler 螺旋、直线和贝塞尔曲线）来推测被遮挡部分的形状。</li>
<li data-line="14" dir="auto">这些方法通常只适用于简单形状（如直线或圆形），在复杂场景中效果较差。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="2. Amodal Appearance Completion（非模态外观补全）" dir="auto" class="heading" id="2._Amodal_Appearance_Completion（非模态外观补全）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. Amodal Appearance Completion（非模态外观补全）</h3><div class="heading-children"><div><p dir="auto">非模态外观补全的目标是恢复被遮挡部分的物体表面外观（即 RGB 值），使得物体看起来完整且一致。大多数当前的非模态外观补全方法都依赖于从非模态形状补全中获得的形状掩码，分为以下几种类型：</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>物体中心表示（Object-centric Representations for Toy Datasets）</strong>：
<ul>
<li data-line="2" dir="auto">主要用于简单的玩具数据集，例如 MONet 和 IODINE，使用变分自编码器（VAE）结合注意力机制来重建物体的外观。</li>
<li data-line="3" dir="auto">这些方法主要在颜色对比度明显的简单背景下有效，难以应用于复杂场景。</li>
</ul>
</li>
<li data-line="4" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>特定类别的外观补全（Category-specific Studies）</strong>：
<ul>
<li data-line="6" dir="auto">针对某些特定类型的物体（如车辆或人体）进行非模态外观补全。</li>
<li data-line="7" dir="auto">例如，Yan 等人（2019）提出了一个 GAN-based 的框架，用于恢复被遮挡车辆的外观，而 Zhou 等人（2021）提出了一种用于人体外观补全的 Unet 模型。</li>
</ul>
</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>复杂场景的外观补全（Methods for Complex Scenes）</strong>：
<ul>
<li data-line="10" dir="auto">针对真实场景中较复杂的物体或背景进行外观补全。</li>
<li data-line="11" dir="auto">这些方法通常需要使用合成数据进行训练，如 SeGAN（Ehsani et al., 2018）使用 cGAN 来生成物体的被遮挡部分的外观。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h3 data-heading="3. Order Perception（顺序感知）" dir="auto" class="heading" id="3._Order_Perception（顺序感知）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. Order Perception（顺序感知）</h3><div class="heading-children"><div><p dir="auto">顺序感知是指理解物体在场景中的相互遮挡关系（Occlusion Order）或深度关系（Layer Order），并依此推测物体的排列顺序。顺序感知可以帮助提升形状和外观补全的精度：</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Occlusion Order（遮挡顺序）</strong>：
<ul>
<li data-line="2" dir="auto">通过推测物体间的遮挡关系（如物体 A 遮挡物体 B），来辅助形状和外观补全任务。</li>
<li data-line="3" dir="auto">例如，BCNet（Ke et al., 2021）使用图卷积网络（GCN）将重叠区域划分为不同的遮挡层，从而分离遮挡物和被遮挡物。</li>
</ul>
</li>
<li data-line="4" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>Layer Order（图层顺序）</strong>：
<ul>
<li data-line="6" dir="auto">图层顺序主要用于确定物体的深度关系（前景和背景的关系）。</li>
<li data-line="7" dir="auto">例如，一些研究将场景划分为前景和背景两个层（Dhamo et al., 2019），或使用多图层结构表示更复杂的物体深度关系（Zheng et al., 2021）。</li>
</ul>
</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="3.Datasets and evaluation" dir="auto" class="heading" id="3.Datasets_and_evaluation"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.Datasets and evaluation</h2><div class="heading-children"><div class="heading-wrapper"><h3 data-heading="3.1Data collection" dir="auto" class="heading" id="3.1Data_collection"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.1Data collection</h3><div class="heading-children"><div class="heading-wrapper"><h4 data-heading="1. **Manual Annotation（手动标注）**" dir="auto" class="heading" id="1._**Manual_Annotation（手动标注）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. <strong>Manual Annotation（手动标注）</strong></h4><div class="heading-children"><div><p dir="auto">手动标注是指通过人工方式对图像进行标注，即由人类标注者根据视觉感知能力来预测物体的被遮挡部分，并手动标注这些不可见区域的轮廓或外观。这种方法是图像非模态补全领域中最直接、最传统的标注方式，具有较高的准确性和一致性，但其缺点在于标注成本高且耗时耗力。</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>具体描述</strong>：
<ul>
<li data-line="2" dir="auto">标注者通常需要通过想象物体的被遮挡部分来标记物体的完整轮廓（例如，将物体看作完全可见的形式进行标注），从而生成非模态分割掩码（amodal segmentation mask）或相应的边界框。</li>
<li data-line="3" dir="auto">标注者之间的意见可能会有差异，但 Zhu et al. (2017) 的研究表明，人类标注者在预测被遮挡区域时通常具有较高的一致性。</li>
<li data-line="4" dir="auto">这一方法通常用于生成非模态形状补全（amodal shape completion）和遮挡顺序（occlusion order）的标注数据集。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优点</strong>：
<ul>
<li data-line="7" dir="auto">由于是由人类标注者依据真实的视觉经验进行标注，标注结果往往具有较高的质量和一致性。</li>
<li data-line="8" dir="auto">可以提供复杂场景中不同物体类别的完整形状标注，从而为模型提供精确的非模态分割掩码。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>局限性</strong>：
<ul>
<li data-line="11" dir="auto">标注成本高：手动标注数据集需要投入大量的人工资源和时间。</li>
<li data-line="12" dir="auto">标注结果通常仅限于物体的非模态形状（即掩码或轮廓），无法提供被遮挡区域的 RGB 外观信息。</li>
</ul>
</li>
<li data-line="13" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>代表性数据集</strong>：
<ul>
<li data-line="15" dir="auto">COCOA（Zhu et al., 2017）数据集：手动标注了 COCO 数据集中部分图像的非模态分割掩码和物体间的遮挡顺序。</li>
<li data-line="16" dir="auto">KINS（Qi et al., 2019）数据集：在 KITTI 数据集上进行了车辆和行人的手动标注，生成了非模态分割掩码和相对遮挡顺序信息。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="2. **Clip-art-based Image Composition（基于剪贴画的图像合成）**" dir="auto" class="heading" id="2._**Clip-art-based_Image_Composition（基于剪贴画的图像合成）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. <strong>Clip-art-based Image Composition（基于剪贴画的图像合成）</strong></h4><div class="heading-children"><div><p dir="auto">基于剪贴画的图像合成是一种较为低成本的方式，通过将现有图像中剪切出的物体片段（patches）叠加到目标图像上，从而模拟被遮挡的场景。这种方法可以为模型提供大规模的非模态外观数据，但其生成的图像通常不够真实，并且容易产生伪影（artifacts）。</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>具体描述</strong>：
<ul>
<li data-line="2" dir="auto">研究人员首先将某些目标物体（例如汽车、人物）的图像块从背景中剪切出来，然后将这些图像块叠加到另一幅目标图像中，形成遮挡关系。</li>
<li data-line="3" dir="auto">通过剪贴画合成的方式，可以自动生成物体的非模态分割掩码和完整外观（RGB appearance）。</li>
<li data-line="4" dir="auto">由于物体是从原图中剪切出来的，因此可以在合成图像中准确地提供被遮挡部分的外观数据。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优点</strong>：
<ul>
<li data-line="7" dir="auto">标注成本低：相比手动标注，通过合成方式可以快速生成大规模的非模态数据集。</li>
<li data-line="8" dir="auto">提供了被遮挡区域的完整外观标注，而不仅仅是物体轮廓，从而适用于非模态外观补全任务。</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>局限性</strong>：
<ul>
<li data-line="11" dir="auto">只能处理通过剪贴画叠加产生的遮挡，难以处理复杂背景中的多物体遮挡。</li>
<li data-line="12" dir="auto">合成图像可能会因为拼接过程而出现不自然的边界或纹理伪影，影响训练效果。</li>
<li data-line="13" dir="auto">难以模拟复杂真实场景中的光照变化和深度关系，合成图像的逼真度和多样性不足。</li>
</ul>
</li>
<li data-line="14" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>代表性数据集</strong>：
<ul>
<li data-line="16" dir="auto">D2S Amodal（Follmann et al., 2019）：通过叠加超市商品的图像片段生成了大量具有非模态分割掩码的合成图像。</li>
<li data-line="17" dir="auto">OVD（Yan et al., 2019）：通过将 COCO 数据集中的物体剪贴到车辆图像上，生成了被遮挡车辆的合成图像，并提供了车辆的完整外观标注。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="3. **Synthetic 2D Images from 3D Scenes（基于合成场景的2D图像生成）**" dir="auto" class="heading" id="3._**Synthetic_2D_Images_from_3D_Scenes（基于合成场景的2D图像生成）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. <strong>Synthetic 2D Images from 3D Scenes（基于合成场景的2D图像生成）</strong></h4><div class="heading-children"><div><p dir="auto">这种方式通过构建 3D 场景，然后生成相应的 2D 图像，从而获得完整的物体标注。合成的 3D 场景能够精确地控制物体的遮挡关系、深度信息和 RGB 外观，因此可以生成高质量的标注数据集，并且能够提供物体间的复杂交互信息。</p></div><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>具体描述</strong>：
<ul>
<li data-line="1" dir="auto">首先在 3D 环境中构建一个包含多个物体的场景（如模拟街景或室内场景），然后通过改变视角来生成不同的 2D 图像。</li>
<li data-line="2" dir="auto">这些合成图像可以提供物体的非模态分割掩码、完整的 RGB 外观、深度信息（Depth Map）以及遮挡关系（Occlusion Order）。</li>
<li data-line="3" dir="auto">由于这些图像是从 3D 模型生成的，因此可以轻松获取物体的完整形状和外观，而不受遮挡的影响。</li>
</ul>
</li>
<li data-line="4" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>优点</strong>：
<ul>
<li data-line="6" dir="auto">标注结果高度精确且具有多样性：合成场景中可以任意控制物体的姿态、位置、遮挡关系和光照条件，从而生成具有高度准确性的标注数据。</li>
<li data-line="7" dir="auto">提供丰富的场景信息：如深度图、遮挡顺序、物体间的空间关系等，可以辅助非模态形状和外观补全任务。</li>
</ul>
</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>局限性</strong>：
<ul>
<li data-line="10" dir="auto">合成场景的构建通常需要复杂的3D建模技术和渲染过程，生成数据集的成本较高。</li>
<li data-line="11" dir="auto">合成图像的视觉效果和细节可能与真实场景存在较大差距，导致模型在真实场景中的泛化能力不足。</li>
</ul>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>代表性数据集</strong>：
<ul>
<li data-line="14" dir="auto">SAIL-VOS（Hu et al., 2019）：使用 GTA-V 环境模拟生成了大量室内和室外场景的图像，并对其中的物体进行了完整标注。</li>
<li data-line="15" dir="auto">CSD（Zheng et al., 2021）：在合成的室内场景中生成了具有多遮挡关系的图像，提供了丰富的非模态形状和外观标注。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="4. **总结**" dir="auto" class="heading" id="4._**总结**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. <strong>总结</strong></h4><div class="heading-children"><div><p dir="auto">这一部分系统性地总结了图像非模态补全任务中常用的数据集生成方式，并详细分析了每种方式的优点和局限性。对于不同的研究任务，可以根据任务需求选择合适的数据集构建方法。</p></div><div><ul>
<li data-line="0" dir="auto"><strong>手动标注</strong> 适用于需要高质量、精确形状标注的任务，但成本高昂。</li>
<li data-line="1" dir="auto"><strong>基于剪贴画的图像合成</strong> 适用于大规模数据集的生成，但生成图像的质量受限于拼接过程。</li>
<li data-line="2" dir="auto"><strong>基于 3D 场景的合成图像</strong> 能够提供高质量的标注数据，但在视觉效果上可能与真实场景存在差距。</li>
</ul></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="3.2 Datasets" dir="auto" class="heading" id="3.2_Datasets"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.2 Datasets</h3><div class="heading-children"><div class="heading-wrapper"><h4 data-heading="1. **数据集的分类与应用场景**" dir="auto" class="heading" id="1._**数据集的分类与应用场景**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>1. <strong>数据集的分类与应用场景</strong></h4><div class="heading-children"><div><p dir="auto">该部分将现有数据集分为三大类，分别是：</p></div><div><ol>
<li data-line="0" dir="auto"><strong>Common Objects Datasets（通用物体数据集）</strong></li>
<li data-line="1" dir="auto"><strong>Vehicle Datasets（车辆数据集）</strong></li>
<li data-line="2" dir="auto"><strong>Specialized Datasets（专用数据集：人体、室内场景等）</strong></li>
</ol></div></div></div><div class="heading-wrapper"><h4 data-heading="2. **Common Objects Datasets（通用物体数据集）**" dir="auto" class="heading" id="2._**Common_Objects_Datasets（通用物体数据集）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>2. <strong>Common Objects Datasets（通用物体数据集）</strong></h4><div class="heading-children"><div><p dir="auto">这些数据集通常包含多种物体类别（例如日常用品、动物、人类等），并且提供了被遮挡物体的完整形状或外观标注。该类数据集旨在提供多类别、多场景的标注，用于模型在通用场景中的训练和评估。</p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>COCOA（COCO Amodal）——Zhu et al., 2017</strong>：</p>
<ul>
<li data-line="2" dir="auto"><strong>来源</strong>：基于 COCO 数据集（Common Objects in Context）生成。</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="4" dir="auto">提供 <strong>非模态实例分割掩码</strong>（amodal segmentation mask），即包含物体被遮挡部分的完整分割轮廓。</li>
<li data-line="5" dir="auto">标注了物体之间的 <strong>遮挡顺序（occlusion order）</strong>。</li>
</ul>
</li>
<li data-line="6" dir="auto"><strong>应用场景</strong>：该数据集可用于通用物体的非模态形状补全及遮挡顺序推理任务。</li>
<li data-line="7" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>局限性</strong>：
<ul>
<li data-line="8" dir="auto">标注内容较为单一，仅提供物体形状和遮挡顺序信息，而不包含被遮挡部分的 RGB 外观。</li>
</ul>
</li>
</ul>
</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>COCOA-cls（COCOA with Class-specific Labels）——Follmann et al., 2019</strong>：</p>
<ul>
<li data-line="11" dir="auto"><strong>来源</strong>：基于 COCOA 数据集，但为物体提供了 <strong>类别特定的非模态标注</strong>（class-specific annotations）。</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="13" dir="auto">提供每个物体的类别标签，以及每类物体的非模态分割掩码。</li>
</ul>
</li>
<li data-line="14" dir="auto"><strong>应用场景</strong>：用于多物体类别的非模态分割和类别识别任务。</li>
</ul>
</li>
<li data-line="15" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>InstaOrder——Lee and Park, 2022</strong>：</p>
<ul>
<li data-line="17" dir="auto"><strong>来源</strong>：同样基于 COCO 数据集，但专注于物体间的几何顺序标注。</li>
<li data-line="18" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="19" dir="auto">提供 <strong>双向遮挡顺序（Bidirectional Occlusion Order）</strong> 和物体间的 <strong>深度顺序（Depth Order）</strong>。</li>
</ul>
</li>
<li data-line="20" dir="auto"><strong>局限性</strong>：仅包含 <strong>模态分割掩码</strong>（modal segmentation masks），而非完整的非模态分割信息。</li>
</ul>
</li>
<li data-line="21" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>D2S Amodal——Follmann et al., 2019</strong>：</p>
<ul>
<li data-line="23" dir="auto"><strong>来源</strong>：基于 Densely Segmented Supermarket (D2S) 数据集进行生成。</li>
<li data-line="24" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="25" dir="auto">提供超市商品的非模态掩码（amodal masks），所有物体被人工合成遮挡关系。</li>
</ul>
</li>
<li data-line="26" dir="auto"><strong>应用场景</strong>：该数据集用于超市商品及日常用品的遮挡关系分析与非模态分割。</li>
</ul>
</li>
</ol></div><div><hr></div></div></div><div class="heading-wrapper"><h4 data-heading="3. **Vehicle Datasets（车辆数据集）**" dir="auto" class="heading" id="3._**Vehicle_Datasets（车辆数据集）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3. <strong>Vehicle Datasets（车辆数据集）</strong></h4><div class="heading-children"><div><p dir="auto">车辆数据集专注于交通场景中车辆的非模态补全，包括被遮挡车辆的形状、外观、遮挡顺序等。由于车辆类别的形状和结构相对稳定，因此这些数据集主要用于训练和评估车辆形状和外观的补全模型。</p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>KINS（KITTI INStance）——Qi et al., 2019</strong>：</p>
<ul>
<li data-line="2" dir="auto"><strong>来源</strong>：基于 KITTI 数据集的交通场景图像。</li>
<li data-line="3" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="4" dir="auto">提供车辆和行人的 <strong>非模态分割掩码</strong> 及物体间的 <strong>相对遮挡顺序（Relative Occlusion Order）</strong>。</li>
</ul>
</li>
<li data-line="5" dir="auto"><strong>应用场景</strong>：适用于城市交通场景中的车辆形状补全和遮挡顺序推理任务。</li>
</ul>
</li>
<li data-line="6" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>OVD（Occluded Vehicle Dataset）——Yan et al., 2019</strong>：</p>
<ul>
<li data-line="8" dir="auto"><strong>来源</strong>：基于 Cars 数据集，通过合成方法生成。</li>
<li data-line="9" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="10" dir="auto">提供被遮挡车辆的 <strong>完整 RGB 外观（RGB appearance）</strong>，以及合成遮挡物体的位置和类别。</li>
</ul>
</li>
<li data-line="11" dir="auto"><strong>应用场景</strong>：用于训练车辆的外观补全模型，并评估模型对复杂遮挡场景的泛化能力。</li>
</ul>
</li>
<li data-line="12" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>Amodal Cityscapes——Breitenstein and Fingscheidt, 2022</strong>：</p>
<ul>
<li data-line="14" dir="auto"><strong>来源</strong>：基于 Cityscapes 数据集生成。</li>
<li data-line="15" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="16" dir="auto">提供城市交通场景中车辆和行人的 <strong>非模态分割掩码</strong>，以及遮挡顺序标注。</li>
</ul>
</li>
<li data-line="17" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>特点</strong>：
<ul>
<li data-line="18" dir="auto">模拟复杂交通场景中的遮挡关系（如行人穿行、车辆重叠等），标注精细度较高。</li>
</ul>
</li>
</ul>
</li>
<li data-line="19" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>KITTI-360-APS 和 BDD100K-APS——Mohan and Valada, 2022a</strong>：</p>
<ul>
<li data-line="21" dir="auto"><strong>来源</strong>：分别基于 KITTI-360 和 BDD100K 数据集进行扩展。</li>
<li data-line="22" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="23" dir="auto">提供城市交通场景中的 <strong>非模态全景标注（amodal panoptic annotations）</strong>。</li>
<li data-line="24" dir="auto">标注包括 "stuff" 类（如道路、建筑物）和 "thing" 类（如车辆、行人）。</li>
</ul>
</li>
<li data-line="25" dir="auto"><strong>应用场景</strong>：用于复杂交通场景中不同类别的遮挡与顺序推理任务。</li>
</ul>
</li>
</ol></div><div><hr></div></div></div><div class="heading-wrapper"><h4 data-heading="4. **Specialized Datasets（专用数据集：人体、室内场景等）**" dir="auto" class="heading" id="4._**Specialized_Datasets（专用数据集：人体、室内场景等）**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. <strong>Specialized Datasets（专用数据集：人体、室内场景等）</strong></h4><div class="heading-children"><div><p dir="auto">这些数据集专注于特定类别（如人体）或特定场景（如室内环境），为特定任务（如人体姿态补全或室内场景分割）提供详细标注。</p></div><div><ol>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>AHP（Amodal Human Parsing）——Zhou et al., 2021</strong>：</p>
<ul>
<li data-line="2" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="3" dir="auto">提供 <strong>人体姿态和外观的非模态分割掩码及外观信息</strong>。</li>
</ul>
</li>
<li data-line="4" dir="auto"><strong>应用场景</strong>：用于人体姿态补全和遮挡推理任务。</li>
</ul>
</li>
<li data-line="5" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>
<p><strong>CSD（Composite Scene Dataset）——Zheng et al., 2021</strong>：</p>
<ul>
<li data-line="7" dir="auto"><strong>来源</strong>：在 3D 合成室内场景中生成。</li>
<li data-line="8" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>标注类型</strong>：
<ul>
<li data-line="9" dir="auto">提供室内场景中物体的 <strong>非模态掩码、RGB 外观及遮挡顺序</strong>。</li>
</ul>
</li>
<li data-line="10" dir="auto"><strong>特点</strong>：适用于复杂室内场景中的物体补全与遮挡顺序推理。</li>
</ul>
</li>
</ol></div></div></div><div class="heading-wrapper"><h4 data-heading="5. **总结**" dir="auto" class="heading" id="5._**总结**"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5. <strong>总结</strong></h4><div class="heading-children"><div><p dir="auto">论文通过对现有非模态补全数据集的分类与总结，为不同场景和任务提供了详细的数据集参考。这些数据集在标注类型、应用场景、遮挡关系和标注精细度上各有侧重，可用于不同任务的模型训练和评估。</p></div><div><p dir="auto">论文的 <strong>3.3. Evaluation metrics and latest benchmark results on the amodal datasets（评估指标与最新基准结果）</strong> 部分，主要讨论了图像非模态补全（image amodal completion）任务中常用的评估指标以及现有数据集上的基准模型表现。由于非模态补全涉及到恢复被遮挡区域的物体形状、外观及遮挡关系，因此在评估时需要考虑形状、外观、深度关系、遮挡顺序等多个方面的指标。该部分为研究人员提供了一个系统的框架，帮助他们更好地理解现有模型的优劣，并为未来研究提供参考。</p></div></div></div></div></div><div class="heading-wrapper"><h3 data-heading="3.3Evaluation metrics and latest benchmark results on the amodal datasets（评估指标与最新基准结果）" dir="auto" class="heading" id="3.3Evaluation_metrics_and_latest_benchmark_results_on_the_amodal_datasets（评估指标与最新基准结果）"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.3Evaluation metrics and latest benchmark results on the amodal datasets（评估指标与最新基准结果）</h3><div class="heading-children"><div><p dir="auto">在非模态补全任务中，模型的性能通常通过以下几种主要评估指标来衡量。根据任务目标和具体应用场景，这些评估指标可分为针对 <strong>形状补全</strong>、<strong>外观补全</strong> 以及 <strong>顺序推理</strong> 的不同指标。以下是对主要指标的详细描述：</p></div><div class="heading-wrapper"><h4 data-heading="3.3.1 Amodal Shape Completion" dir="auto" class="heading" id="3.3.1_Amodal_Shape_Completion"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.3.1 Amodal Shape Completion</h4><div class="heading-children"><div><ul>
<li data-line="0" dir="auto"><strong>目标</strong>：针对遮挡形状的补全任务，常用评估指标包括 <code>IoU（Intersection over Union）</code>、<code>mIoU（Mean IoU）</code>、<code>Precision（精确率）</code>、<code>Recall（召回率）</code>、<code>F1-score</code> 及 <code>mAP（mean Average Precision）</code>。通常 mAP 是最常用的指标。</li>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>数据集</strong>：主要使用 COCOA 和 KINS 两个数据集来评估基准表现，分别在 <code>Amodal Instance Segmentation</code> 任务中测试多种方法的表现：
<ul>
<li data-line="2" dir="auto">如 ORCNN、ASN（Mask-RCNN）、PCNet、BCNet、CSDNet 及 Shape Prior 方法。</li>
</ul>
</li>
<li data-line="3" dir="auto"><strong>结果</strong>：在 COCOA 和 KINS 数据集上，不同方法取得的 mAP 分数有所不同，最高分为 Shape Prior 方法在 COCOA 数据集上达到 35.4%。</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="3.3.2 Amodal Appearance Completion" dir="auto" class="heading" id="3.3.2_Amodal_Appearance_Completion"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.3.2 Amodal Appearance Completion</h4><div class="heading-children"><div><ul>
<li data-line="0" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>目标</strong>：该任务的重点在于预测物体不可见部分的外观和细节纹理。为了评估图像的生成质量，研究者常采用以下指标：
<ul>
<li data-line="1" dir="auto"><strong>MSE（Mean Square Error）</strong>、<strong>RMSE（Root Mean Square Error）</strong>、<strong>PSNR（Peak Signal-to-Noise Ratio）</strong>、以及更复杂的视觉感知度量标准，如 <strong>SSIM（Structural Similarity Index）</strong> 和 <strong>FID（Fréchet Inception Distance）</strong>。</li>
</ul>
</li>
<li data-line="2" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>数据集</strong>：该任务中使用的主要数据集包括：
<ul>
<li data-line="3" dir="auto"><strong>CSD（Zheng et al., 2021）</strong> 和 <strong>AHP（Zhou et al., 2021）</strong>，分别测试不同模型在生成图像外观上的表现。</li>
<li data-line="4" dir="auto">在 SSIM 和 PSNR 评估中，CSDNet 达到了 0.91 和 35.24 的得分。</li>
</ul>
</li>
</ul></div></div></div><div class="heading-wrapper"><h4 data-heading="3.3.3 Order Perception" dir="auto" class="heading" id="3.3.3_Order_Perception"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>3.3.3 Order Perception</h4><div class="heading-children"><div><ul>
<li data-line="0" dir="auto"><strong>目标</strong>：研究任务包括遮挡顺序（Occlusion Order）和图层顺序（Layer Order）的感知与预测。</li>
<li data-line="1" dir="auto"><div class="list-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><strong>遮挡顺序</strong>：针对成对物体实例的遮挡顺序，使用以下评估指标：
<ul>
<li data-line="2" dir="auto"><strong>Pairwise Depth Ordering Accuracy（成对深度顺序精度）</strong>、<strong>Precision、Recall、F1-score</strong> 以及 <strong>Pairwise Ordering Average Precision</strong>。</li>
<li data-line="3" dir="auto">数据集包括 COCOA、KINS 和 InstaOrder 数据集，用于测试不同模型的顺序恢复能力。</li>
</ul>
</li>
<li data-line="4" dir="auto"><strong>图层顺序</strong>：图层顺序任务中使用 <code>RMSE（均方根误差）</code>、<code>REL（相对误差）</code> 及多种基于阈值的准确性指标。此外，还可采用 <code>Damerow–Levenstein 距离</code> 来计算预测顺序与真实顺序之间的最小操作次数。</li>
</ul></div></div></div></div></div></div></div><div class="heading-wrapper"><h2 data-heading="4. 应用总结" dir="auto" class="heading" id="4._应用总结"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>4. 应用总结</h2><div class="heading-children"><div><p dir="auto">图像 Amodal 补全技术在许多实际应用中具有重要价值，涵盖了自动生成 Amodal 标注、场景编辑和重构、减弱现实（Diminished Reality）、机器人抓取系统、自动驾驶和新视角合成等场景。</p></div><div><ol>
<li data-line="0" dir="auto"><strong>自动生成 Amodal 标注</strong>： 手动标注 Amodal 数据集非常昂贵且耗时，因为这要求标注者对不可见部分进行合理推测。为了降低标注成本，现有的方法能够通过将已有的 Modal 标注转化为伪 Amodal 标注，从而生成高质量的 Amodal 掩码。这些自动生成的掩码甚至在某些任务上超越了手动标注，提供了比人工标注更细致和自然的结果。</li>
<li data-line="2" dir="auto"><strong>场景编辑和重构</strong>： Amodal 补全任务能够将场景中的物体与背景进行分离，同时还提供了物体间的层次顺序信息。这些信息能够用于对象顺序的交换、物体移除或复制，从而使场景编辑更为方便，特别是在移除大型遮挡物时（如移除照片中的人群或车辆）。Amodal 补全技术还在隐私保护应用中大有作为，例如移除敏感物体时，可以更自然地填充被遮挡的背景部分。</li>
<li data-line="4" dir="auto"><strong>减弱现实（Diminished Reality，DR）</strong>： DR 技术通过去除场景中的不必要实体来提升用户体验，是 AR（增强现实）技术的补充。在应用场景中，DR 技术能够移除用户不希望看到的障碍物，并填补缺失的图像信息。这对于例如虚拟家具展示等场景非常有用，能够有效提升产品展示效果。</li>
<li data-line="6" dir="auto"><strong>机器人抓取系统</strong>： 在实际环境中，机器人经常需要应对被遮挡的目标物体。Amodal 补全技术能够帮助机器人理解遮挡关系和目标物体的形状，从而在复杂环境中更好地规划抓取策略，并避免在操作过程中发生碰撞。</li>
<li data-line="8" dir="auto"><strong>自动驾驶</strong>： 自动驾驶系统需要对道路上的各种物体（如车辆、行人和障碍物）进行识别，而这些物体可能被部分遮挡。Amodal 补全能够帮助自动驾驶系统预测物体的完整形状，避免因仅依赖可见部分而导致的错误识别，从而提升驾驶安全性。</li>
<li data-line="10" dir="auto"><strong>新视角合成（Novel View Synthesis）</strong>： Amodal 补全能够生成遮挡物后方的完整视角信息，这对于新视角图像生成（如从单一视图合成三维图像）至关重要。该技术通过填补被遮挡区域，从而生成更加完整的 3D 视图，应用于场景重建和虚拟现实中。</li>
</ol></div></div></div><div class="heading-wrapper"><h2 data-heading="5.Open issues and future directions" dir="auto" class="heading" id="5.Open_issues_and_future_directions"><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div>5.Open issues and future directions</h2><div class="heading-children"><div><p dir="auto">尽管现有的研究在图像Amodal补全领域取得了显著进展，但在实际应用中仍然存在许多挑战。以下是对这些挑战以及未来研究方向的总结：</p></div><div><ol>
<li data-line="0" dir="auto"><strong>Amodal数据集的挑战</strong>：现有Amodal数据集的标注主要依赖于人工推测，对于遮挡物体的不可见部分，标注具有一定的主观性。这使得生成更接近真实情况的标注成为一个重要研究方向。未来可能需要开发更真实的数据集，并采用新的成像手段（如X射线）来改进标注的准确性。同时，开发能够完整展示物体不可见部分的真实数据集仍然是一项待解决的挑战。</li>
<li data-line="2" dir="auto"><strong>输出结果的多样性</strong>：物体隐藏部分的多样性是Amodal补全任务的自然问题。当只有部分物体可见时，合理的补全方式可能有多种不同可能性。因此，未来研究应探索如何在形状补全、外观补全以及顺序感知任务中更好地表示这些多样性，并考虑多种可能的结果模式。</li>
<li data-line="4" dir="auto"><strong>更好的可视化表示</strong>：目前大多数Amodal补全的结果都仅限于二维表示。然而，如何更有效地展示额外推测的信息仍需要进一步研究。未来的研究可能考虑将这些信息构建成2.5D或3D表示，从而提供更直观的结果。</li>
<li data-line="6" dir="auto"><strong>联合解决多个子问题</strong>：部分研究已经尝试了对多个子问题的联合解决，但很少有研究对联合解决这些问题的重要性进行深入讨论。未来的研究方向可能集中于阐明子问题间的相互依赖关系，并通过消融实验验证联合解决方案的效果。</li>
<li data-line="8" dir="auto"><strong>一致的性能度量标准</strong>：由于该领域尚处于早期阶段，目前对于各个Amodal补全任务缺乏一致的评价标准。因此，未来需要针对每个任务制定统一的性能评估标准，以便更好地进行不同方法间的比较。</li>
<li data-line="10" dir="auto"><strong>实时模型</strong>：尽管目前在Amodal补全的理论研究中取得了较大进展，但实际应用仍需要能够在复杂场景中实时处理的模型。未来的研究应重点考虑如何在保持模型性能的同时，提高其实时处理能力。</li>
<li data-line="12" dir="auto"><strong>与其他任务或上下文的集成</strong>：Amodal补全任务与目标检测、语义分割和深度估计等其他任务的结合具有很大的潜力。未来的研究可以探索多任务联合的模型设计，并利用其他任务的优势来提升Amodal补全的表现。</li>
</ol></div><div class="mod-footer"></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-handle"></div><div class="sidebar-topbar"><div class="topbar-content"></div><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder">
		<div class="graph-view-container">
			<div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div>
			<canvas id="graph-canvas" class="hide" width="512px" height="512px"></canvas>
		</div>
		</div></div><div class="tree-container mod-root nav-folder tree-item outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button" aria-label="Collapse All"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area tree-item-children nav-folder-children"><div class="tree-item mod-tree-folder nav-folder mod-collapsible is-collapsed" style="display: none;"></div><div class="tree-item" data-depth="1"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#论文：Image amodal completion _A survey"><div class="tree-item-contents heading-link" heading-name="论文：Image amodal completion _A survey"><span class="tree-item-title">论文：Image amodal completion _A survey</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="2"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#1._Introduction"><div class="tree-item-contents heading-link" heading-name="1. Introduction"><span class="tree-item-title">1. 
Introduction
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#2.Taxonomy_of_problems"><div class="tree-item-contents heading-link" heading-name="2.Taxonomy of problems"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">2.Taxonomy of problems</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#1._Amodal_Shape_Completion（非模态形状补全）"><div class="tree-item-contents heading-link" heading-name="1. Amodal Shape Completion（非模态形状补全）"><span class="tree-item-title">1. 
Amodal Shape Completion（非模态形状补全）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#2._Amodal_Appearance_Completion（非模态外观补全）"><div class="tree-item-contents heading-link" heading-name="2. Amodal Appearance Completion（非模态外观补全）"><span class="tree-item-title">2. 
Amodal Appearance Completion（非模态外观补全）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3._Order_Perception（顺序感知）"><div class="tree-item-contents heading-link" heading-name="3. Order Perception（顺序感知）"><span class="tree-item-title">3. 
Order Perception（顺序感知）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="2"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.Datasets_and_evaluation"><div class="tree-item-contents heading-link" heading-name="3.Datasets and evaluation"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.Datasets and evaluation</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.1Data_collection"><div class="tree-item-contents heading-link" heading-name="3.1Data collection"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.1Data collection</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#1._**Manual_Annotation（手动标注）**"><div class="tree-item-contents heading-link" heading-name="1. **Manual Annotation（手动标注）**"><span class="tree-item-title">1. 
Manual Annotation（手动标注）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#2._**Clip-art-based_Image_Composition（基于剪贴画的图像合成）**"><div class="tree-item-contents heading-link" heading-name="2. **Clip-art-based Image Composition（基于剪贴画的图像合成）**"><span class="tree-item-title">2. 
Clip-art-based Image Composition（基于剪贴画的图像合成）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3._**Synthetic_2D_Images_from_3D_Scenes（基于合成场景的2D图像生成）**"><div class="tree-item-contents heading-link" heading-name="3. **Synthetic 2D Images from 3D Scenes（基于合成场景的2D图像生成）**"><span class="tree-item-title">3. 
Synthetic 2D Images from 3D Scenes（基于合成场景的2D图像生成）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#4._**总结**"><div class="tree-item-contents heading-link" heading-name="4. **总结**"><span class="tree-item-title">4. 
总结
</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.2_Datasets"><div class="tree-item-contents heading-link" heading-name="3.2 Datasets"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.2 Datasets</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#1._**数据集的分类与应用场景**"><div class="tree-item-contents heading-link" heading-name="1. **数据集的分类与应用场景**"><span class="tree-item-title">1. 
数据集的分类与应用场景
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#2._**Common_Objects_Datasets（通用物体数据集）**"><div class="tree-item-contents heading-link" heading-name="2. **Common Objects Datasets（通用物体数据集）**"><span class="tree-item-title">2. 
Common Objects Datasets（通用物体数据集）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3._**Vehicle_Datasets（车辆数据集）**"><div class="tree-item-contents heading-link" heading-name="3. **Vehicle Datasets（车辆数据集）**"><span class="tree-item-title">3. 
Vehicle Datasets（车辆数据集）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#4._**Specialized_Datasets（专用数据集：人体、室内场景等）**"><div class="tree-item-contents heading-link" heading-name="4. **Specialized Datasets（专用数据集：人体、室内场景等）**"><span class="tree-item-title">4. 
Specialized Datasets（专用数据集：人体、室内场景等）
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#5._**总结**"><div class="tree-item-contents heading-link" heading-name="5. **总结**"><span class="tree-item-title">5. 
总结
</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div><div class="tree-item mod-collapsible" data-depth="3"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.3Evaluation_metrics_and_latest_benchmark_results_on_the_amodal_datasets（评估指标与最新基准结果）"><div class="tree-item-contents heading-link" heading-name="3.3Evaluation metrics and latest benchmark results on the amodal datasets（评估指标与最新基准结果）"><div class="collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">3.3Evaluation metrics and latest benchmark results on the amodal datasets（评估指标与最新基准结果）</span></div></a><div class="tree-item-children nav-folder-children"><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.3.1_Amodal_Shape_Completion"><div class="tree-item-contents heading-link" heading-name="3.3.1 Amodal Shape Completion"><span class="tree-item-title">3.3.1 Amodal Shape Completion</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.3.2_Amodal_Appearance_Completion"><div class="tree-item-contents heading-link" heading-name="3.3.2 Amodal Appearance Completion"><span class="tree-item-title">3.3.2 Amodal Appearance Completion</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="4"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#3.3.3_Order_Perception"><div class="tree-item-contents heading-link" heading-name="3.3.3 Order Perception"><span class="tree-item-title">3.3.3 Order Perception</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#4._应用总结"><div class="tree-item-contents heading-link" heading-name="4. 应用总结"><span class="tree-item-title">4. 
应用总结
</span></div></a><div class="tree-item-children nav-folder-children"></div></div><div class="tree-item" data-depth="2"><a class="tree-link" href="非模态分割\论文：image-amodal-completion-_a-survey.html#5.Open_issues_and_future_directions"><div class="tree-item-contents heading-link" heading-name="5.Open issues and future directions"><span class="tree-item-title">5.Open issues and future directions</span></div></a><div class="tree-item-children nav-folder-children"></div></div></div></div></div></div></div><script defer="">let rs = document.querySelector(".sidebar-right"); rs.classList.add("is-collapsed"); if (window.innerWidth > 768) rs.classList.remove("is-collapsed"); rs.style.setProperty("--sidebar-width", localStorage.getItem("sidebar-right-width"));</script></div></div></body></html>